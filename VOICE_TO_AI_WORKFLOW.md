# Voice-to-AI Workflow Documentation

## ğŸ¤â¡ï¸ğŸ¤– Complete Voice-to-AI Pipeline

### Current Workflow:

```
1. ğŸ¤ User speaks into microphone/uploads audio file
   â†“
2. ğŸ”„ Audio gets processed by Whisper-tiny model
   â†“ 
3. ğŸ“ Speech is transcribed to English text
   â†“
4. ğŸ§  Text is sent to your main model: "model/Whisper-psychology-gemma-3-1b"
   â†“
5. ğŸ” FAISS searches relevant documents for context
   â†“
6. ğŸ’¬ Main model generates psychological response
   â†“
7. ğŸ“º Response is displayed in chat
   â†“
8. ğŸ”Š (Optional) Response can be converted to speech via TTS
```

### Technical Implementation:

#### Step 1-3: Speech-to-Text
```python
# Audio processing with Whisper-tiny
transcribed_text = transcribe_audio(
    audio_bytes, 
    st.session_state.whisper_model,     # whisper-tiny model
    st.session_state.whisper_processor
)
```

#### Step 4-6: AI Processing  
```python
# Main model processing
answer, sources, metadata = process_medical_query(
    transcribed_text,                    # Your speech as text
    st.session_state.faiss_index,       # Document search
    st.session_state.embedding_model,
    st.session_state.optimal_docs,
    st.session_state.model,             # YOUR MAIN MODEL HERE
    st.session_state.tokenizer,         # model/Whisper-psychology-gemma-3-1b
    **generation_params
)
```

#### Step 7-8: Response Display
```python
# Add to chat and optionally convert to speech
st.session_state.messages.append({
    "role": "assistant", 
    "content": answer,      # Response from your main model
    "sources": sources,
    "metadata": metadata
})
```

### Models Used:

1. **Speech-to-Text**: `stt-model/whisper-tiny/`
   - Converts your voice to English text
   - Language: English only (forced)

2. **Main AI Model**: `model/Whisper-psychology-gemma-3-1b/`  â­ **YOUR MODEL**
   - Processes the transcribed text
   - Generates psychological responses
   - Uses RAG with FAISS for context

3. **Text-to-Speech**: `tts-model/Kokoro-82M/`
   - Converts AI response back to speech
   - Currently uses placeholder implementation

4. **Document Search**: `faiss_index/`
   - Provides context for better responses

### Usage:

1. **Click the microphone button** ğŸ¤
2. **Speak your mental health question**
3. **Click "ğŸ”„ Transcribe Audio"**
4. **Watch the complete pipeline work automatically:**
   - Your speech â†’ Text
   - Text â†’ Your AI model
   - AI response â†’ Chat
   - Optional: Response â†’ Speech

### What happens when you transcribe:

âœ… **Immediate automatic processing** - No manual steps needed!
âœ… **Your speech text goes directly to your main model**
âœ… **Full psychiatric AI response is generated**
âœ… **Complete conversation appears in chat**
âœ… **Optional TTS for audio response**

The system now automatically sends your transcribed speech to your `model/Whisper-psychology-gemma-3-1b` model and gets a full AI response without any additional steps!
